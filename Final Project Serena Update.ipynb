{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 311_cases csv\n",
    "\n",
    "cases = pd.read_csv(\"311_Cases.csv\", sep = \",\", dtype = \"str\")\n",
    "\n",
    "cases = cases.drop(cases.columns[range(20,47)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Datetime \n",
    "date_columns = [\"Opened\", \"Closed\", \"Updated\"]\n",
    "\n",
    "# Convert specified columns into datetime format \n",
    "### NOTE: datetime changes depending on where you run the program. Change accordingly ###\n",
    "\n",
    "cases[date_columns] = cases[date_columns].apply(pd.to_datetime, format = \"%m/%d/%Y %I:%M:%S %p\", errors = 'coerce')\n",
    "# cases[date_columns] = cases[date_columns].apply(pd.to_datetime, format = \"%m/%d/%Y %H:%M\", errors = 'coerce')\n",
    "\n",
    "# Find Closed - Opened in hours\n",
    "time_elapsed = (cases[\"Closed\"] - cases[\"Opened\"])/np.timedelta64(1, 'h')\n",
    "\n",
    "# Insert Time Elapsed into cases df\n",
    "cases.insert(3, \"Time Elapsed\", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cases = cases.dropna()\n",
    "\n",
    "new_cases = cases[cases['Time Elapsed'] > 0]\n",
    "\n",
    "new_cases.sort_values(by = \"Time Elapsed\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Names\n",
    "cat_name = ['Status','Category','Neighborhood']\n",
    "\n",
    "new_cases = pd.DataFrame(new_cases['Time Elapsed'])\n",
    "\n",
    "for i in cat_name: \n",
    "    x = pd.get_dummies(cases[i], drop_first=True)\n",
    "    new_cases = pd.concat([new_cases, x], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram of Time Elapsed \n",
    "\n",
    "y = new_cases['Time Elapsed']\n",
    "\n",
    "plt.hist(y, 50)\n",
    "\n",
    "plt.xlabel(\"Time Elapsed\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Density of Time Elapsed\")\n",
    "\n",
    "# As you can see, the data is heavily skewed towards 0, but we have a few observations that are within the 10,000 range.\n",
    "# Apply log transformation to recenter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram of Log(Time Elapsed)\n",
    "\n",
    "plt.hist(np.log(y), 50)\n",
    "\n",
    "plt.xlabel(\"Log(Time Elapsed)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Log Density of Time Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Lecture 16 notebook \n",
    "\n",
    "X = new_cases.loc[:, new_cases.columns != 'Time Elapsed']\n",
    "\n",
    "y = np.log(new_cases['Time Elapsed'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# I chose not to transform the data because most of the data is categorical (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha= 0.0001, max_iter=10000000, tol=0.000001)\n",
    "\n",
    "# Fit the models\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"lasso score:\", lasso.score(X_test, y_test))\n",
    "print(\"lasso MSE:\", mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = linear_model.Ridge()\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"ridge score:\",ridge.score(X_test, y_test))\n",
    "print(\"ridge MSE:\", mean_squared_error(y_test, ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "\n",
    "dt_reg = regressor.fit(X_train,y_train)\n",
    "\n",
    "dt_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different Way of Measuring Accuracy \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)     # get predicted samples \n",
    "    errors = abs(test_labels - predictions)        # abs value of residuals\n",
    "    MAE = sum(errors)/len(errors)\n",
    "    MSE = mean_squared_error(y_test, predictions)\n",
    "    print('Model Performance')\n",
    "    print('MAE: {:0.4f} log(hrs).'.format(np.mean(errors)))\n",
    "    print('MSE = ', MSE)\n",
    "    print('R2 = ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Accuracy\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pprint import pprint\n",
    "\n",
    "base_model = RandomForestRegressor()\n",
    "\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(base_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 70, stop = 90, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "min_samples_split = list(range(2,10))\n",
    "max_depth = [2,3, 4,5]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with RandomSearchCV \n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 500,\n",
    "                               cv = 3, verbose=2, n_jobs = -1)\n",
    "\n",
    "rand_reg = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rand_reg.best_params_)\n",
    "\n",
    "\n",
    "rand_accuracy = evaluate(rand_reg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter Tuning \n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(70,90,2)), \n",
    "    'max_features': list(range(2,8,2))\n",
    "}\n",
    "\n",
    "# Most important: \n",
    "# - n_estimators = # of trees \n",
    "# - max_features = # of feature considered at each leaf node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_grid = grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1)\n",
    "\n",
    "rf_reg = rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_reg.best_params_)\n",
    "\n",
    "\n",
    "grid_accuracy = evaluate(rf_reg, X_test, y_test)\n",
    "\n",
    "# Source: https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "# Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# Source: https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot real vs predicted\n",
    "\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize = (10,8), sharey= True)\n",
    "# fig.suptitle('Vertically stacked subplots')\n",
    "ax1.plot(range(len(y_test)), y_test)\n",
    "ax2.plot(range(len(rf_pred)), rf_pred)\n",
    "ax1.set_title(\"True Data Values\")\n",
    "ax2.set_title(\"Predicted Values\")\n",
    "ax1.set_ylabel(\"Log Hours Elapsed\")\n",
    "ax2.set_ylabel(\"Log Hours Elapsed\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Source: https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/subplots_demo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_q2 = new_cases\n",
    "\n",
    "# open = 0\n",
    "# close = 1\n",
    "\n",
    "# 24hrs pass, if time elapsed is negative == close; if time elapsed positive == open\n",
    "\n",
    "cases_q2['Time Elapsed'] = cases_q2['Time Elapsed'] - 24\n",
    "cases_q2.loc[cases_q2['Time Elapsed'] >= 0, 'Time Elapsed'] = 0\n",
    "cases_q2.loc[cases_q2['Time Elapsed'] < 0, 'Time Elapsed'] = 1\n",
    "\n",
    "cases_q2 = cases_q2.rename(columns = {'Time Elapsed': \"Status_24hrs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cases_q2.loc[:, cases_q2.columns != 'Status_24hrs']\n",
    "\n",
    "y = cases_q2['Status_24hrs']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', solver='liblinear', random_state=1)\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "C = [10, 1, .1, .001]\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for c in C:\n",
    "    clf = LogisticRegression(penalty='l1', C=c, solver='liblinear', random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train.append(clf.score(X_train, y_train))\n",
    "    test.append(clf.score(X_test, y_test))\n",
    "    y_pred = clf.predict(X_test) \n",
    "    print(confusion_matrix(y_test, y_pred)) \n",
    "\n",
    "    \n",
    "results = pd.DataFrame({'C': C, 'Training Accuracy': train, 'Test Accuracy':test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(random_state=1)\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['liblinear']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lr = GridSearchCV(lr2, param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "grid_lr.score(X_test, y_test)\n",
    "\n",
    "y_pred = grid_lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Decision Tree Classifier (1 decision tree)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "\n",
    "classifier = DecisionTreeClassifier()  \n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test) \n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Create param grid\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators' : list(range(10,101,5)),\n",
    "     'max_features' : list(range(6,32,2))}]\n",
    "                       \n",
    "\n",
    "clf = GridSearchCV(clf, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "best_clf = clf.fit(X_train, y_train)\n",
    "y_pred = best_clf.predict(X_test) \n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(best_clf.best_params_)\n",
    "\n",
    "print('Accuracy: ', best_clf.score(X_test, y_test))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "models = [best_clf, classifier, grid_lr, lr]\n",
    "name = ['Random Forest', 'Decision Tree', 'GridSearchCV LR','Default Logistic Regression']\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "\n",
    "for i in range(len(models)):\n",
    "    plot_roc_curve(models[i], X_test, y_test, ax=ax, name = name[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
