{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 311_cases csv\n",
    "\n",
    "cases = pd.read_csv(\"311_Cases.csv\", sep = \",\", dtype = \"str\")\n",
    "\n",
    "cases = cases.drop(cases.columns[range(20,47)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Datetime \n",
    "date_columns = [\"Opened\", \"Closed\", \"Updated\"]\n",
    "\n",
    "# Convert specified columns into datetime format \n",
    "### NOTE: datetime changes depending on where you run the program. Change accordingly ###\n",
    "\n",
    "cases[date_columns] = cases[date_columns].apply(pd.to_datetime, format = \"%m/%d/%Y %I:%M:%S %p\", errors = 'coerce')\n",
    "# cases[date_columns] = cases[date_columns].apply(pd.to_datetime, format = \"%m/%d/%Y %H:%M\", errors = 'coerce')\n",
    "\n",
    "# Find Closed - Opened in hours\n",
    "time_elapsed = (cases[\"Closed\"] - cases[\"Opened\"])/np.timedelta64(1, 'h')\n",
    "\n",
    "# Insert Time Elapsed into cases df\n",
    "cases.insert(3, \"Time Elapsed\", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cases = cases.dropna()\n",
    "\n",
    "new_cases = cases[cases['Time Elapsed'] > 0]\n",
    "\n",
    "new_cases.sort_values(by = \"Time Elapsed\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Names\n",
    "cat_name = ['Status','Category','Neighborhood']\n",
    "\n",
    "new_cases = pd.DataFrame(new_cases['Time Elapsed'])\n",
    "\n",
    "for i in cat_name: \n",
    "    x = pd.get_dummies(cases[i], drop_first=True)\n",
    "    new_cases = pd.concat([new_cases, x], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram of Time Elapsed \n",
    "\n",
    "y = new_cases['Time Elapsed']\n",
    "\n",
    "plt.hist(y, 50)\n",
    "\n",
    "plt.xlabel(\"Time Elapsed\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Density of Time Elapsed\")\n",
    "\n",
    "# As you can see, the data is heavily skewed towards 0, but we have a few observations that are within the 10,000 range.\n",
    "# Apply log transformation to recenter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram of Log(Time Elapsed)\n",
    "\n",
    "plt.hist(np.log(y), 50)\n",
    "\n",
    "plt.xlabel(\"Log(Time Elapsed)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Log Density of Time Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Lecture 16 notebook \n",
    "\n",
    "X = new_cases.loc[:, new_cases.columns != 'Time Elapsed']\n",
    "\n",
    "y = np.log(new_cases['Time Elapsed'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# I chose not to transform the data because most of the data is categorical (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha= 0.0001, max_iter=10000000, tol=0.000001)\n",
    "\n",
    "# Fit the models\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"lasso score:\", lasso.score(X_test, y_test))\n",
    "print(\"lasso MSE:\", mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = linear_model.Ridge()\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"ridge score:\",ridge.score(X_test, y_test))\n",
    "print(\"ridge MSE:\", mean_squared_error(y_test, ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "\n",
    "dt_reg = regressor.fit(X_train,y_train)\n",
    "\n",
    "dt_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different Way of Measuring Accuracy \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)     # get predicted samples \n",
    "    errors = abs(test_labels - predictions)        # abs value of residuals\n",
    "    MAE = sum(errors)/len(errors)\n",
    "    MSE = mean_squared_error(y_test, predictions)\n",
    "    print('Model Performance')\n",
    "    print('MAE: {:0.4f} log(hrs).'.format(np.mean(errors)))\n",
    "    print('MSE = ', MSE)\n",
    "    print('R2 = ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Accuracy\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pprint import pprint\n",
    "\n",
    "base_model = RandomForestRegressor()\n",
    "\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(base_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 70, stop = 90, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "min_samples_split = list(range(2,10))\n",
    "max_depth = [2,3, 4,5]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with RandomSearchCV \n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 500,\n",
    "                               cv = 3, verbose=2, n_jobs = -1)\n",
    "\n",
    "rand_reg = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rand_reg.best_params_)\n",
    "\n",
    "\n",
    "rand_accuracy = evaluate(rand_reg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter Tuning \n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(70,90,2)), \n",
    "    'max_features': list(range(2,8,2))\n",
    "}\n",
    "\n",
    "# Most important: \n",
    "# - n_estimators = # of trees \n",
    "# - max_features = # of feature considered at each leaf node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_grid = grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1)\n",
    "\n",
    "rf_reg = rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_reg.best_params_)\n",
    "\n",
    "\n",
    "grid_accuracy = evaluate(rf_reg, X_test, y_test)\n",
    "\n",
    "# Source: https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "# Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# Source: https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot real vs predicted\n",
    "\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize = (10,8), sharey= True)\n",
    "# fig.suptitle('Vertically stacked subplots')\n",
    "ax1.plot(range(len(y_test)), y_test)\n",
    "ax2.plot(range(len(rf_pred)), rf_pred)\n",
    "ax1.set_title(\"True Data Values\")\n",
    "ax2.set_title(\"Predicted Values\")\n",
    "ax1.set_ylabel(\"Log Hours Elapsed\")\n",
    "ax2.set_ylabel(\"Log Hours Elapsed\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Source: https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/subplots_demo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_q2 = new_cases\n",
    "\n",
    "# open = 0\n",
    "# close = 1\n",
    "\n",
    "# 24hrs pass, if time elapsed is negative == close; if time elapsed positive == open\n",
    "\n",
    "cases_q2['Time Elapsed'] = cases_q2['Time Elapsed'] - 24\n",
    "cases_q2.loc[cases_q2['Time Elapsed'] >= 0, 'Time Elapsed'] = 0\n",
    "cases_q2.loc[cases_q2['Time Elapsed'] < 0, 'Time Elapsed'] = 1\n",
    "\n",
    "cases_q2 = cases_q2.rename(columns = {'Time Elapsed': \"Status_24hrs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cases_q2.loc[:, cases_q2.columns != 'Status_24hrs']\n",
    "\n",
    "y = cases_q2['Status_24hrs']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', solver='liblinear', random_state=1)\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "C = [10, 1, .1, .001]\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for c in C:\n",
    "    clf = LogisticRegression(penalty='l1', C=c, solver='liblinear', random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train.append(clf.score(X_train, y_train))\n",
    "    test.append(clf.score(X_test, y_test))\n",
    "    y_pred = clf.predict(X_test) \n",
    "    print(confusion_matrix(y_test, y_pred)) \n",
    "\n",
    "    \n",
    "results = pd.DataFrame({'C': C, 'Training Accuracy': train, 'Test Accuracy':test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(random_state=1)\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['liblinear']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lr = GridSearchCV(lr2, param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "grid_lr.score(X_test, y_test)\n",
    "\n",
    "y_pred = grid_lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Decision Tree Classifier (1 decision tree)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "\n",
    "classifier = DecisionTreeClassifier()  \n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test) \n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Create param grid\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators' : list(range(10,101,5)),\n",
    "     'max_features' : list(range(6,32,2))}]\n",
    "                       \n",
    "\n",
    "clf = GridSearchCV(clf, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "best_clf = clf.fit(X_train, y_train)\n",
    "y_pred = best_clf.predict(X_test) \n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(best_clf.best_params_)\n",
    "\n",
    "print('Accuracy: ', best_clf.score(X_test, y_test))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "models = [best_clf, classifier, grid_lr, lr]\n",
    "name = ['Random Forest', 'Decision Tree', 'GridSearchCV LR','Default Logistic Regression']\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "\n",
    "for i in range(len(models)):\n",
    "    plot_roc_curve(models[i], X_test, y_test, ax=ax, name = name[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases3 = cases.dropna()\n",
    "# subset data\n",
    "cases_q3 = cases3.drop(['Request Details','CaseID','Media URL','Closed','Updated',\n",
    "                       'Status Notes','Point','Address'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by day and night \n",
    "# day = 0; night = 1\n",
    "cases_q3 = cases_q3.set_index(pd.to_datetime(cases_q3['Opened']))\n",
    "cases_q3[\"day_night\"] = 1\n",
    "cases_q3.loc[cases_q3.between_time(\"06:00\", \"18:00\").index, \"day_night\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced data\n",
    "cases_q3[\"day_night\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variables for categorical columns\n",
    "status = pd.get_dummies(cases_q3['Status'],drop_first=True)\n",
    "category = pd.get_dummies(cases_q3['Category'],drop_first=True)\n",
    "request = pd.get_dummies(cases_q3['Request Type'],drop_first=True)\n",
    "agency = pd.get_dummies(cases_q3['Responsible Agency'],drop_first=True)\n",
    "street = pd.get_dummies(cases_q3['Street'],drop_first=True)\n",
    "supervisor = pd.get_dummies(cases_q3['Supervisor District'],drop_first=True)\n",
    "neighbor = pd.get_dummies(cases_q3['Neighborhood'],drop_first=True)\n",
    "police = pd.get_dummies(cases_q3['Police District'],drop_first=True)\n",
    "source = pd.get_dummies(cases_q3['Source'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = pd.concat([cases_q3,status,category,request,agency,street,supervisor,neighbor,police,source],axis=1)\n",
    "\n",
    "# dropping the original columns\n",
    "df_cases = df_cases.drop(['Status','Category','Request Type','Time Elapsed','Opened','Responsible Agency','Street',\n",
    "              'Supervisor District','Neighborhood','Police District',\n",
    "              'Opened','Source'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension Reduction & Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cases.loc[:,df_cases.columns != 'day_night']\n",
    "\n",
    "# create y from rating\n",
    "y = df_cases['day_night']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression After PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA the data except day_night column\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2,random_state=1)\n",
    "pca_data = pca.fit_transform(df_cases.loc[:,df_cases.columns != 'day_night'])\n",
    "\n",
    "# cumulative explained variance \n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into test and training sets, with 70% of samples being put into the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression after PCA\n",
    "clf = LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print('Coefficient of each feature:', clf.coef_)\n",
    "print('Training accuracy:', clf.score(X_train, y_train))\n",
    "print('Test accuracy:', clf.score(X_test, y_test))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy \n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score\n",
    "\n",
    "y_predict = clf.predict(X_train) \n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_train, y_predict))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Without PCA (Original Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into test and training sets, with 70% of samples being put into the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression after PCA\n",
    "clf = LogisticRegression(penalty='l2', solver='liblinear',class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print('Training accuracy:', clf.score(X_train, y_train))\n",
    "print('Test accuracy:', clf.score(X_test, y_test))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy \n",
    "y_predict = clf.predict(X_train) \n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_train, y_predict))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cases['Request Details'].str.split('-').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating car brands by price levels\n",
    "high = ['BMW','Mercedes','Audi','Tesla','Porsche','Ferrari','Land Rover',\n",
    "        'Lamborghini','Maserati','Jaguar','Cadillac']\n",
    "med = ['Acura','Lexus','Mini','Infiniti','Volvo']\n",
    "low = ['Honda','Kia','Toyota','Hyundai','Ford','Nissan','Chevrolet',\n",
    "       'Volkswagon','VW','Jeep','Mazda','Subaru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe with request type and car brands extracted from request details column\n",
    "df_cars = pd.concat([cars,cases['Request Type']],axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = df_cars['Request Details']\n",
    "\n",
    "# labeling appropriate price levels for requests based on car brands\n",
    "df_cars.loc[req.str.contains('|'.join(high)),'Price Level'] = 'High'\n",
    "df_cars.loc[req.str.contains('|'.join(med)),'Price Level'] = 'Medium'\n",
    "df_cars.loc[req.str.contains('|'.join(low)),'Price Level'] = 'Low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cleaned dataset\n",
    "data_cars = df_cars.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to see if there's a relationship between price level and request type\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "tfidf = tf_idf.fit_transform(data_cars['Request Type']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2,random_state=1)\n",
    "tfidf_pca = pca.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = data_cars.index[data_cars['Price Level'] == 'High']\n",
    "med = data_cars.index[data_cars['Price Level'] == 'Medium']\n",
    "low = data_cars.index[data_cars['Price Level'] == 'Low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by price level and cluster request type\n",
    "from collections import Counter\n",
    "# density of request type by high priced cars\n",
    "# density is shown by the size of the points\n",
    "x = tfidf_pca[high,0]\n",
    "y = tfidf_pca[high,1]\n",
    "c = Counter(zip(x,y)) # count same points\n",
    "s = [c[(xx,yy)] for xx,yy in zip(x,y)] # scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=s)\n",
    "plt.title('Q4: PCA for High Price Level')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge kpca and data_cars by price level (high)\n",
    "x_coord = c.most_common(1)[0][0][0]\n",
    "y_coord = c.most_common(1)[0][0][1]\n",
    "\n",
    "car_index = np.where((tfidf_pca[high,0] == x_coord) & (tfidf_pca[high,1] == y_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request type with the highest count\n",
    "high_car = data_cars.loc[data_cars['Price Level'] == 'High']\n",
    "high_car.reset_index(drop=True, inplace=True)\n",
    "high_car.loc[(car_index[0][0]), data_cars.columns == 'Request Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density of request type by medium priced cars\n",
    "x = tfidf_pca[med,0]\n",
    "y = tfidf_pca[med,1]\n",
    "c = Counter(zip(x,y))\n",
    "s = [c[(xx,yy)] for xx,yy in zip(x,y)]\n",
    "\n",
    "plt.scatter(x, y, s=s)\n",
    "plt.title('Q4: PCA for Medium Price Level')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge kpca and data_cars by price level (med)\n",
    "x_coord = c.most_common(1)[0][0][0]\n",
    "y_coord = c.most_common(1)[0][0][1]\n",
    "\n",
    "car_index = np.where((tfidf_pca[med,0] == x_coord) & (tfidf_pca[med,1] == y_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request type with the highest count\n",
    "med_car = data_cars.loc[data_cars['Price Level'] == 'Medium']\n",
    "med_car.reset_index(drop=True, inplace=True)\n",
    "med_car.loc[(car_index[0][0]), data_cars.columns == 'Request Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density of request type by low priced cars\n",
    "x = tfidf_pca[low,0]\n",
    "y = tfidf_pca[low,1]\n",
    "c = Counter(zip(x,y))\n",
    "s = [c[(xx,yy)] for xx,yy in zip(x,y)] # change multiplier \n",
    "\n",
    "plt.scatter(x, y, s=s)\n",
    "plt.title('Q4: PCA for Low Price Level')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge kpca and data_cars by price level (low)\n",
    "x_coord = c.most_common(1)[0][0][0]\n",
    "y_coord = c.most_common(1)[0][0][1]\n",
    "\n",
    "car_index = np.where((tfidf_pca[low,0] == x_coord) & (tfidf_pca[low,1] == y_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request type with the highest count\n",
    "low_car = data_cars.loc[data_cars['Price Level'] == 'Low']\n",
    "low_car.reset_index(drop=True, inplace=True)\n",
    "low_car.loc[(car_index[0][0]), data_cars.columns == 'Request Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NaNs from \"Closed\" column\n",
    "cases7 = cases[cases['Closed'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_q7 = cases7.drop(['Request Details','CaseID','Media URL','Closed','Updated',\n",
    "                       'Status Notes','Point','Address'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variables for categorical columns\n",
    "status = pd.get_dummies(cases_q7['Status'],drop_first=True)\n",
    "category = pd.get_dummies(cases_q7['Category'],drop_first=True)\n",
    "agency = pd.get_dummies(cases_q7['Responsible Agency'],drop_first=True)\n",
    "street = pd.get_dummies(cases_q7['Street'],drop_first=True)\n",
    "supervisor = pd.get_dummies(cases_q7['Supervisor District'],drop_first=True)\n",
    "neighbor = pd.get_dummies(cases_q7['Neighborhood'],drop_first=True)\n",
    "police = pd.get_dummies(cases_q7['Police District'],drop_first=True)\n",
    "source = pd.get_dummies(cases_q7['Source'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7_cases = pd.concat([cases_q7,status,category,agency,street,supervisor,neighbor,police,source],axis=1)\n",
    "\n",
    "# dropping the original columns\n",
    "df7_cases = df7_cases.drop(['Status','Category','Responsible Agency','Street',\n",
    "              'Supervisor District','Neighborhood','Police District',\n",
    "              'Opened','Source'],axis=1)\n",
    "df7_cases = df7_cases.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting seven most common request types\n",
    "na_data = df7_cases.groupby('Request Type').agg('count')['Latitude'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df7_cases.loc[:,df7_cases.columns != 'Request Type']\n",
    "\n",
    "# create y from rating\n",
    "y = df7_cases['Request Type']\n",
    "\n",
    "# scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(X)\n",
    "pca_q7 = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = pca_q7,\n",
    "                           columns = ['principal component 1', 'principal component 2'])\n",
    "principalDf.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "finalDf = pd.concat([principalDf, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA plot\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = list(na_data.index) # target values\n",
    "colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'magenta','black']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['Request Type'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets,loc = 'upper center',bbox_to_anchor = (1.45, 0.8), shadow = True, ncol = 1)\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
